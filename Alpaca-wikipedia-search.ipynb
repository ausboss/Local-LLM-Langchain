{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "f1fcbe22",
      "metadata": {},
      "source": [
       "# Local Model Notebook loader\n",
       "## This is for people who want to test langchain or other agent/agi related code in a notebook\n",
       "\n",
       "\n",
       "## ‚ö†Ô∏èLlama-cpp usersü¶ô‚ö†Ô∏è\n",
       "If you are using Llama-cpp you can skip down to the llama cpp cell\n",
       "\n",
       "If your Llama uses gpu then dont skip\n",
       "# Text-generation-webui related code\n",
       "## Load Required Libraries and Modules\n",
       "The first step is to load all the required libraries and modules:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6df86eb8",
      "metadata": {},
      "outputs": [],
      "source": [
       "!pip install langchain"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4b411355",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\n",
         "===================================BUG REPORT===================================\n",
         "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
         "================================================================================\n",
         "CUDA SETUP: CUDA runtime path found: C:\\Users\\admin\\Documents\\oobabooga-windows\\installer_files\\env\\bin\\cudart64_110.dll\n",
         "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
         "CUDA SETUP: Detected CUDA version 117\n",
         "CUDA SETUP: Loading binary C:\\Users\\admin\\Documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
        ]
       }
      ],
      "source": [
       "import sys\n",
       "sys.argv = [sys.argv[0]]\n",
       "import os\n",
       "import re\n",
       "import time\n",
       "import json\n",
       "from pathlib import Path\n",
       "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
       "from langchain.llms import HuggingFacePipeline\n",
       "from langchain import PromptTemplate, LLMChain\n",
       "sys.path.append(str(Path().resolve().parent / \"modules\"))\n",
       "from modules import api, chat, shared, training, ui\n",
       "from modules.html_generator import chat_html_wrapper\n",
       "from modules.LoRA import add_lora_to_model\n",
       "from modules.models import load_model, load_soft_prompt\n",
       "from modules.text_generation import generate_reply, stop_everything_event\n",
       "import torch\n",
       "torch.cuda.set_device(0)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "636bf6a3",
      "metadata": {},
      "source": [
       "# Parameters and command-line flags\n",
       "\n",
       "input your command line arguments like you would when launching server.py [complete list](https://github.com/oobabooga/text-generation-webui#basic-settings)\n",
       "\n",
       "Example: --auto-devices --wbits 4 --groupsize 128 --no-stream\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "id": "58084747",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Enter args string: --auto-devices --wbits 4 --groupsize 128 --no-stream\n"
        ]
       }
      ],
      "source": [
       "from modules.shared import parser\n",
       "\n",
       "def parse_input_string(input_string):\n",
       "    input_args = input_string.split()\n",
       "    return parser.parse_args(input_args)\n",
       "\n",
       "input_string = input('Enter args string: ')\n",
       "shared.args = parse_input_string(input_string)\n",
       "# Load custom settings from a JSON file\n",
       "settings_file = None\n",
       "if shared.args.settings is not None and Path(shared.args.settings).exists():\n",
       "    settings_file = Path(shared.args.settings)\n",
       "elif Path('settings.json').exists():\n",
       "    settings_file = Path('settings.json')\n",
       "\n",
       "if settings_file is not None:\n",
       "    print(f\"Loading settings from {settings_file}...\")\n",
       "    new_settings = json.loads(open(settings_file, 'r').read())\n",
       "    for item in new_settings:\n",
       "        shared.settings[item] = new_settings[item]\n",
       "\n",
       "shared.settings['seed'] = -1\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "e363387c",
      "metadata": {},
      "source": [
       "# Choose your model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2d68b2a5",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "The following models are available:\n",
         "\n",
         "1. anon8231489123_gpt4-x-alpaca-13b-native-4bit-128g\n",
         "2. chavinlo_alpaca-native\n",
         "3. gozfarb_oasst-llama13b-4bit-128g\n",
         "4. llama-13b-ggml-q4_0\n",
         "5. llama-30b-4bit-128g\n",
         "6. llama-30b-sft-oa-alpaca-epoch-2\n",
         "7. llama-7b\n",
         "8. MetaIX_GPT4-X-Alpaca-30B-Int4\n",
         "9. vicuna-13b-GPTQ-4bit-128g\n",
         "\n",
         "Which one do you want to load? 1-9\n",
         "\n",
         "6\n",
         "\n"
        ]
       }
      ],
      "source": [
       "# Function to get available models\n",
       "def get_available_models():\n",
       "    if shared.args.flexgen:\n",
       "        return sorted([re.sub('-np$', '', item.name) for item in list(Path(f'{shared.args.model_dir}/').glob('*')) if item.name.endswith('-np')], key=str.lower)\n",
       "    else:\n",
       "        return sorted([re.sub('.pth$', '', item.name) for item in list(Path(f'{shared.args.model_dir}/').glob('*')) if not item.name.endswith(('.txt', '-np', '.pt', '.json'))], key=str.lower)\n",
       "\n",
       "# Get the list of available models\n",
       "available_models = get_available_models()\n",
       "\n",
       "# Set the model name\n",
       "if shared.args.model is not None:\n",
       "    shared.model_name = shared.args.model\n",
       "else:\n",
       "    if len(available_models) == 0:\n",
       "        print('No models are available! Please download at least one.')\n",
       "        sys.exit(0)\n",
       "    elif len(available_models) == 1:\n",
       "        i = 0\n",
       "    else:\n",
       "        print('The following models are available:\\n')\n",
       "        for i, model in enumerate(available_models):\n",
       "            print(f'{i+1}. {model}')\n",
       "        print(f'\\nWhich one do you want to load? 1-{len(available_models)}\\n')\n",
       "        i = int(input()) - 1\n",
       "        print()\n",
       "    shared.model_name = available_models[i]\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "2b4771cd",
      "metadata": {},
      "source": [
       "# Load Model and Tokenizer"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "id": "02f6ae98",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Loading llama-30b-sft-oa-alpaca-epoch-2...\n",
         "Found the following quantized model: models\\llama-30b-sft-oa-alpaca-epoch-2\\llama-30b-sft-ao-alpaca-epoch-2-hf-int4-128g.safetensors\n",
         "Loading model ...\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "C:\\Users\\admin\\Documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
         "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
         "C:\\Users\\admin\\Documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
         "  return self.fget.__get__(instance, owner)()\n",
         "C:\\Users\\admin\\Documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
         "  storage = cls(wrap_storage=untyped_storage)\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Done.\n",
         "Loaded the model in 22.44 seconds.\n"
        ]
       }
      ],
      "source": [
       "# Load the model and tokenizer\n",
       "shared.model, shared.tokenizer = load_model(shared.model_name)\n",
       "\n",
       "# Add Lora to the model if specified\n",
       "if shared.args.lora:\n",
       "    add_lora_to_model(shared.args.lora)\n",
       "\n",
       "# Set up the tokenizer and model variables\n",
       "tokenizer = shared.tokenizer\n",
       "base_model = shared.model"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "c2b4c523",
      "metadata": {},
      "source": [
       "## We create a text-generation pipeline with the specified parameters:\n",
       "Feel free to change these to best fit your model/usage"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 74,
      "id": "3857d1a8",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a text-generation pipeline with the specified parameters\n",
       "pipe = pipeline(\n",
       "    \"text-generation\",\n",
       "    model=base_model, \n",
       "    tokenizer=tokenizer,\n",
       "    device=0,\n",
       "    max_length=1200,\n",
       "    temperature=0.6,\n",
       "    top_p=0.95,\n",
       "    repetition_penalty=1.1\n",
       ")\n",
       "\n",
       "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "ee2996aa",
      "metadata": {},
      "source": [
       "## The model is now loaded\n",
       "Run the next cell to test it"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 75,
      "id": "35f9419d",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "1. Toilet World\n",
         "2. The Porcelain Palace\n",
         "3. Bathroom Boutique\n"
        ]
       }
      ],
      "source": [
       "text = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction:\n",
       "Provide 3 potential names for a business that sells toilets.\n",
       "\n",
       "### Response:\n",
       "\n",
       "\"\"\"\n",
       "print(llm(text))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "c516f055",
      "metadata": {},
      "source": [
       "## If that worked then you can skip down to the Lanchain part\n",
       "##\n",
       "\n",
       "# ü¶ôü¶ôü¶ôü¶ôü¶ôü¶ôü¶ôü¶ô\n",
       "# ü¶ôLlama-cpp usersü¶ô\n",
       "# ü¶ôü¶ôü¶ôü¶ôü¶ôü¶ôü¶ôü¶ô\n",
       "## If you are just using llama-cpp then follow these steps\n",
       "\n",
       "A folder containingg your cpp .bin file should be located in the models folder \n",
       "\n",
       "Example: \"./models/Alpaca-7B-ggml-4bit-LoRA-merged/ggml-model-q4_0.bin\"\n",
       "\n",
       "## Install and Import dependencies"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "216c2668",
      "metadata": {},
      "outputs": [],
      "source": [
       "!pip install llama-cpp-python\n",
       "!pip install langchain"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f5c1334",
      "metadata": {},
      "outputs": [],
      "source": [
       "from langchain.llms import LlamaCpp\n",
       "from langchain import PromptTemplate, LLMChain"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "eb8478bd",
      "metadata": {},
      "source": [
       "# Select Model / Load Model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "83932f62",
      "metadata": {},
      "outputs": [],
      "source": [
       "model_dir = \"./models\"\n",
       "import os\n",
       "# get a list of all folders in the models directory\n",
       "model_folders = [f for f in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir, f))]\n",
       "\n",
       "# print the list of model names with their index starting at 1\n",
       "for i, model_name in enumerate(model_folders):\n",
       "    print(f\"{i+1}. {model_name}\")\n",
       "\n",
       "# ask the user to select a model by number\n",
       "selected_index = int(input(\"Enter the number of the model to select: \")) - 1\n",
       "selected_model = model_folders[selected_index]\n",
       "\n",
       "# check if the selected model contains a .bin file and save the path if it does\n",
       "model_bin = None\n",
       "for file in os.listdir(os.path.join(model_dir, selected_model)):\n",
       "    if file.endswith(\".bin\"):\n",
       "        model_bin = os.path.join(model_dir, selected_model, file)\n",
       "        break\n",
       "\n",
       "if model_bin:\n",
       "    print(f\"Selected model binary: {model_bin}\")\n",
       "else:\n",
       "    print(\"No .bin file found in selected model directory.\")\n",
       "    \n",
       "llm = LlamaCpp(model_path=model_bin)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d0a7c8b8",
      "metadata": {},
      "source": [
       "## The model is now loaded\n",
       "Run the next cell to test it"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "b73d88ba",
      "metadata": {},
      "outputs": [],
      "source": [
       "text = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction:\n",
       "Provide 3 potential names for a business that sells toilets.\n",
       "\n",
       "### Response:\n",
       "\n",
       "\"\"\"\n",
       "print(llm(text))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7bc3e44b",
      "metadata": {},
      "source": [
       "## If that worked then you can proceed\n",
       "##\n",
       "# üëáüëáüëáüëáüëáüëáüëáüëáüëáüëáüëáüëáüëáüëáüëáüëáüëá\n",
       "# ‚õìÔ∏èBegginning of Langchain section‚õìÔ∏è\n",
       "I stole some of the code from [this colab](https://colab.research.google.com/drive/1VOwJpcZqOXag-ZXi-52ibOx6L5Pw-YJi#scrollTo=nu-AmhDLEK0h) that goes with [this video](https://www.youtube.com/watch?v=LbT1yp6quS8) by Patrick Loeber. I recommend subscribing."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f877c78c",
      "metadata": {},
      "source": [
       "## Custom LLM Agent - Google Search\n",
       "Google search requires a SERAPI API KEY\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c19db4aa",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Requirement already satisfied: langchain in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (0.0.137)\n",
         "Requirement already satisfied: SQLAlchemy<2,>=1 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.4.47)\n",
         "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (3.8.4)\n",
         "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (6.0)\n",
         "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (0.5.7)\n",
         "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (2.28.2)\n",
         "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (4.0.2)\n",
         "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.2.4)\n",
         "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (8.2.2)\n",
         "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.10.7)\n",
         "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.23.5)\n",
         "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
         "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
         "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
         "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
         "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
         "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
         "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
         "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
         "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
         "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
         "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
         "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
         "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
         "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
         "Requirement already satisfied: packaging>=17.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
         "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
         "Requirement already satisfied: google-search-results in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (2.4.2)\n",
         "Requirement already satisfied: requests in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from google-search-results) (2.28.2)\n",
         "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests->google-search-results) (1.26.15)\n",
         "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests->google-search-results) (2022.12.7)\n",
         "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests->google-search-results) (3.1.0)\n",
         "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\documents\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests->google-search-results) (3.4)\n"
        ]
       }
      ],
      "source": [
       "!pip install langchain\n",
       "!pip install google-search-results"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ac05700b",
      "metadata": {},
      "outputs": [],
      "source": [
       "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
       "from langchain.prompts import BaseChatPromptTemplate\n",
       "from langchain import SerpAPIWrapper, LLMChain\n",
       "from langchain.chat_models import ChatOpenAI\n",
       "from typing import List, Union\n",
       "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
       "import re\n",
       "import os"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "17814e56",
      "metadata": {},
      "source": [
       "## Add SERAPI API Key to the cell below\n",
       "You can get a free one here https://serpapi.com/users/sign_up?plan=free\n",
       "It does require phone number. You can skip down to the wikipedia code if you dont want to mess with api keys."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0740ce8f",
      "metadata": {},
      "outputs": [],
      "source": [
       "os.environ[\"SERPAPI_API_KEY\"] = \"\""
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7d777497",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define which tools the agent can use to answer user queries\n",
       "search = SerpAPIWrapper()\n",
       "tools = [\n",
       "    Tool(\n",
       "        name = \"Search\",\n",
       "        func=search.run,\n",
       "        description=\"useful for when you need to answer questions about current events\"\n",
       "    )\n",
       "]"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "fa0c3148",
      "metadata": {},
      "source": [
       "## Set up the base template. \n",
       "This template is based on what kind of instructions llama was trained on. More info [here](https://github.com/tatsu-lab/stanford_alpaca#data-release)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 11,
      "id": "382cb605",
      "metadata": {},
      "outputs": [],
      "source": [
       "template = \"\"\"Please read the following instruction and input, and respond with the appropriate actions and final answer.\n",
       "\n",
       "### Instruction:\n",
       "Answer the following questions as best you can. When giving the Final Answer speak like a pirate.  You have access to the following tools: {tools}\n",
       "\n",
       "### Input:\n",
       "Question: {input}\n",
       "{agent_scratchpad}\n",
       "\n",
       "\n",
       "Make sure to include the following elements in your response:\n",
       "- Thought process\n",
       "- Action (the name of the tool, one word only, {tool_names})\n",
       "- Action Input (the input to the action)\n",
       "- Observation (the result of the action; include the input context here if necessary)\n",
       "- Final Answer (the final answer to: {input})\n",
       "\"\"\""
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d9c228ed",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Set up a prompt template\n",
       "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
       "    # The template to use\n",
       "    template: str\n",
       "    # The list of tools available\n",
       "    tools: List[Tool]\n",
       "    \n",
       "    def format_messages(self, **kwargs) -> str:\n",
       "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
       "        # Format them in a particular way\n",
       "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
       "        thoughts = \"\"\n",
       "        for action, observation in intermediate_steps:\n",
       "            thoughts += action.log\n",
       "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
       "        # Set the agent_scratchpad variable to that value\n",
       "        kwargs[\"agent_scratchpad\"] = thoughts\n",
       "        # Create a tools variable from the list of tools provided\n",
       "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
       "        # Create a list of tool names for the tools provided\n",
       "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
       "        formatted = self.template.format(**kwargs)\n",
       "        return [HumanMessage(content=formatted)]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 13,
      "id": "763b2861",
      "metadata": {},
      "outputs": [],
      "source": [
       "prompt = CustomPromptTemplate(\n",
       "    template=template,\n",
       "    tools=tools,\n",
       "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
       "    # This includes the `intermediate_steps` variable because that is needed\n",
       "    input_variables=[\"input\", \"intermediate_steps\"]\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 14,
      "id": "998d5fbc",
      "metadata": {},
      "outputs": [],
      "source": [
       "class CustomOutputParser(AgentOutputParser):\n",
       "    \n",
       "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
       "        # Check if agent should finish\n",
       "        if \"Final Answer:\" in llm_output:\n",
       "            return AgentFinish(\n",
       "                # Return values is generally always a dictionary with a single `output` key\n",
       "                # It is not recommended to try anything else at the moment :)\n",
       "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
       "                log=llm_output,\n",
       "            )\n",
       "        # Parse out the action and action input\n",
       "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
       "        match = re.search(regex, llm_output, re.DOTALL)\n",
       "        if not match:\n",
       "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
       "        action = match.group(1).strip()\n",
       "        action_input = match.group(2)\n",
       "        # Return the action and action input\n",
       "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1756e6d",
      "metadata": {},
      "outputs": [],
      "source": [
       "output_parser = CustomOutputParser()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 16,
      "id": "58644ea5",
      "metadata": {},
      "outputs": [],
      "source": [
       "# LLM chain consisting of the LLM and a prompt\n",
       "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2b1b35ed",
      "metadata": {},
      "outputs": [],
      "source": [
       "tool_names = [tool.name for tool in tools]\n",
       "agent = LLMSingleActionAgent(\n",
       "    llm_chain=llm_chain, \n",
       "    output_parser=output_parser,\n",
       "    stop=[\"\\nObservation:\"], \n",
       "    allowed_tools=tool_names\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c5956edd",
      "metadata": {},
      "outputs": [],
      "source": [
       "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d7072183",
      "metadata": {},
      "source": [
       "## üëÅÔ∏èüëÅÔ∏èObserve the results in this cellüëá\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4b3b9546",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\n",
         "\n",
         "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
         "\u001b[32;1m\u001b[1;3m\n",
         "\n",
         "\n",
         "Thought process:\n",
         "Action: Search\n",
         "Action Input: \"metal gear solid 3 snake eater\"\u001b[0m\n",
         "\n",
         "Observation:\u001b[36;1m\u001b[1;3mMetal Gear Solid 3: Snake Eater is a 2004 action-adventure stealth video game developed and published by Konami for the PlayStation 2. It was released in late 2004 in North America and Japan, and in early 2005 in Europe and Australia.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
         "Final Answer: Arrrrrr, it be released on November 17th, 2004.\u001b[0m\n",
         "\n",
         "\u001b[1m> Finished chain.\u001b[0m\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "'Arrrrrr, it be released on November 17th, 2004.'"
         ]
        },
        "execution_count": 20,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "agent_executor.run(\"When did metal gear solid 3 snake eater come out?\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "31f76b4d",
      "metadata": {},
      "source": [
       "## Custom LLM Agent - Wikipedia Search\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df17707",
      "metadata": {
       "scrolled": true
      },
      "outputs": [],
      "source": [
       "!pip install langchain\n",
       "!pip install wikipedia"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 76,
      "id": "c3ae8a1f",
      "metadata": {},
      "outputs": [],
      "source": [
       "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
       "from langchain.prompts import BaseChatPromptTemplate\n",
       "from langchain import SerpAPIWrapper, LLMChain\n",
       "from langchain.chat_models import ChatOpenAI\n",
       "from typing import List, Union\n",
       "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
       "import re\n",
       "import os"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 77,
      "id": "77dd1be7",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define which tools the agent can use to answer user queries\n",
       "from langchain.utilities import WikipediaAPIWrapper\n",
       "wikipedia = WikipediaAPIWrapper()\n",
       "tools = [\n",
       "    Tool(\n",
       "        name = \"Search\",\n",
       "        func=wikipedia.run,\n",
       "        description=\"useful for when you need to look up information about something\"\n",
       "    )\n",
       "]"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "6db78159",
      "metadata": {},
      "source": [
       "## Set up the base template. \n",
       "This template is based on what kind of instructions llama was trained on. More info [here](https://github.com/tatsu-lab/stanford_alpaca#data-release)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 80,
      "id": "dcb4d687",
      "metadata": {},
      "outputs": [],
      "source": [
       "template = \"\"\"Please read the following instruction and input, and respond with the appropriate actions and final answer.\n",
       "\n",
       "### Instruction:\n",
       "Answer the following questions as best you can. When giving the Final Answer speak like a pirate.  You have access to the following tools: {tools}\n",
       "\n",
       "### Input:\n",
       "Question: {input}\n",
       "{agent_scratchpad}\n",
       "\n",
       "\n",
       "Make sure to include the following elements in your response:\n",
       "- Thought process\n",
       "- Action (the name of the tool, one word only, {tool_names})\n",
       "- Action Input (the input to the action)\n",
       "- Observation (the result of the action; include the input context here if necessary)\n",
       "- Final Answer (the final answer to: {input})\n",
       "\"\"\""
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 81,
      "id": "9848f0fe",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Set up a prompt template\n",
       "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
       "    # The template to use\n",
       "    template: str\n",
       "    # The list of tools available\n",
       "    tools: List[Tool]\n",
       "    \n",
       "    def format_messages(self, **kwargs) -> str:\n",
       "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
       "        # Format them in a particular way\n",
       "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
       "        thoughts = \"\"\n",
       "        for action, observation in intermediate_steps:\n",
       "            thoughts += action.log\n",
       "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
       "        # Set the agent_scratchpad variable to that value\n",
       "        kwargs[\"agent_scratchpad\"] = thoughts\n",
       "        # Create a tools variable from the list of tools provided\n",
       "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
       "        # Create a list of tool names for the tools provided\n",
       "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
       "        formatted = self.template.format(**kwargs)\n",
       "        return [HumanMessage(content=formatted)]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 82,
      "id": "98968a60",
      "metadata": {},
      "outputs": [],
      "source": [
       "prompt = CustomPromptTemplate(\n",
       "    template=template,\n",
       "    tools=tools,\n",
       "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
       "    # This includes the `intermediate_steps` variable because that is needed\n",
       "    input_variables=[\"input\", \"intermediate_steps\"]\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 83,
      "id": "b2c93d17",
      "metadata": {},
      "outputs": [],
      "source": [
       "class CustomOutputParser(AgentOutputParser):\n",
       "    \n",
       "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
       "        # Check if agent should finish\n",
       "        if \"Final Answer:\" in llm_output:\n",
       "            return AgentFinish(\n",
       "                # Return values is generally always a dictionary with a single `output` key\n",
       "                # It is not recommended to try anything else at the moment :)\n",
       "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
       "                log=llm_output,\n",
       "            )\n",
       "        # Parse out the action and action input\n",
       "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
       "        match = re.search(regex, llm_output, re.DOTALL)\n",
       "        if not match:\n",
       "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
       "        action = match.group(1).strip()\n",
       "        action_input = match.group(2)\n",
       "        # Return the action and action input\n",
       "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 84,
      "id": "9b3522be",
      "metadata": {},
      "outputs": [],
      "source": [
       "output_parser = CustomOutputParser()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 85,
      "id": "361bac6a",
      "metadata": {},
      "outputs": [],
      "source": [
       "# LLM chain consisting of the LLM and a prompt\n",
       "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 86,
      "id": "a5935085",
      "metadata": {},
      "outputs": [],
      "source": [
       "tool_names = [tool.name for tool in tools]\n",
       "agent = LLMSingleActionAgent(\n",
       "    llm_chain=llm_chain, \n",
       "    output_parser=output_parser,\n",
       "    stop=[\"\\nObservation:\"], \n",
       "    allowed_tools=tool_names\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 87,
      "id": "a5de408d",
      "metadata": {},
      "outputs": [],
      "source": [
       "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 88,
      "id": "c8a98974",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\n",
         "\n",
         "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
         "\u001b[32;1m\u001b[1;3m\n",
         "\n",
         "\n",
         "Thought process:\n",
         "Action: Search\n",
         "Action Input: \"who voiced solid snake in metal gear solid 2?\"\u001b[0m\n",
         "\n",
         "Observation:\u001b[36;1m\u001b[1;3mPage: Metal Gear Solid 3: Snake Eater\n",
         "Summary: Metal Gear Solid 3: Snake Eater is a 2004 action-adventure stealth video game developed and published by Konami for the PlayStation 2. It was released in late 2004 in North America and Japan, and in early 2005 in Europe and Australia. It was the fifth Metal Gear game written and directed by Hideo Kojima and serves as a prequel to the entire Metal Gear series. An expanded edition, titled Metal Gear Solid 3: Subsistence, was released in Japan in late 2005, then in North America, Europe and Australia in 2006. A remastered version of the game was later included in the Metal Gear Solid HD Collection for the PlayStation 3, PlayStation Vita and Xbox 360, while a reworked version, titled Metal Gear Solid: Snake Eater 3D, was released for the Nintendo 3DS in 2012.\n",
         "Set in 1964, 31 years before the events of the original Metal Gear, the story centers on the FOX operative codenamed Naked Snake as he attempts to rescue Russian rocket scientist Nikolai Stepanovich Sokolov, sabotage an experimental superweapon, and assassinate his defected former boss. While previous games were set in a primarily urban environment, Snake Eater adopts a 1960s Soviet jungle setting, with the high tech, near-future trappings of previous Metal Gear Solid games replaced with wilderness. While the environment has changed, the game's focus remains on stealth and infiltration, while retaining the series' self-referential, fourth-wall-breaking sense of humor. The story of Snake Eater is told through numerous cutscenes and radio conversations.\n",
         "Considered one of the greatest video games of all time, Metal Gear Solid 3 was met with critical acclaim and was a commercial success, having sold more than four million copies worldwide as of March 2010.\n",
         "\n",
         "Page: Metal Gear Solid: Portable Ops\n",
         "Summary: Metal Gear Solid: Portable Ops, officially abbreviated MPO, is a 2006 action-adventure stealth video game developed by Kojima Productions and published by Konami for the PlayStation Portable. The game was directed by Masahiro Yamamoto and written by Gakuto Mikumo, with series creator Hideo Kojima acting as producer.While not the first Metal Gear game for the PSP, unlike the previously released Metal Gear Acid and its sequel, as well as the Metal Gear Solid: Digital Graphic Novel, it retains the action-based play mechanics from the mainline series. Set in 1970, six years after the events of Metal Gear Solid 3: Snake Eater, the game follows the exploits of Naked Snake after he finds himself captured in Colombia by the now renegade FOX unit.\n",
         "\n",
         "Page: Metal Gear Solid: The Twin Snakes\n",
         "Summary: Metal Gear Solid: The Twin Snakes is a 2004 action-adventure stealth video game developed by Konami and Silicon Knights and published by Konami for the GameCube. Released in March, the game is a remake of Metal Gear Solid, originally developed by Konami Computer Entertainment Japan for the PlayStation in 1998.\n",
         "The Twin Snakes features graphical improvements over the original, new cutscenes written and directed by Ryuhei Kitamura, and gameplay functions originally introduced in the sequel Metal Gear Solid 2: Sons of Liberty. The game includes a revised translation with re-recorded voice acting, using almost all of the original English voice cast. The game was met with positive reception.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
         "Final Answer:\n",
         "\n",
         "\"Who voiced Solid Snake in Metal Gear Solid 2?\"\n",
         "\n",
         "David Hayter\u001b[0m\n",
         "\n",
         "\u001b[1m> Finished chain.\u001b[0m\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "'\"Who voiced Solid Snake in Metal Gear Solid 2?\"\\n\\nDavid Hayter'"
         ]
        },
        "execution_count": 88,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "agent_executor.run(\"who voiced solid snake in metal gear solid 2?\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e2e41c",
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   