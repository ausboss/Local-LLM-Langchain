{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210157ac",
   "metadata": {},
   "source": [
    "# text-generation-webui related code\n",
    "\n",
    "## Load Required Libraries and Modules\n",
    "The first step is to load all the required libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d580a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (0.0.139)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: gptcache>=0.1.7 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (0.1.10)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: openai in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from gptcache>=0.1.7->langchain) (0.27.4)\n",
      "Requirement already satisfied: cachetools in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from openai->gptcache>=0.1.7->langchain) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from tqdm->openai->gptcache>=0.1.7->langchain) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b411355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: Loading binary C:\\Users\\austi\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\austi\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = [sys.argv[0]]\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "sys.path.append(str(Path().resolve().parent / \"modules\"))\n",
    "from modules import api, chat, shared, training, ui\n",
    "from modules.html_generator import chat_html_wrapper\n",
    "from modules.LoRA import add_lora_to_model\n",
    "from modules.models import load_model, load_soft_prompt\n",
    "from modules.text_generation import generate_reply, stop_everything_event\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773e7b4",
   "metadata": {},
   "source": [
    "Run this cell if you are using your gpu/cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b962a7d",
   "metadata": {},
   "source": [
    "# Parameters and command-line flags\n",
    "\n",
    "input your command line arguments like you would when launching server.py [complete list](https://github.com/oobabooga/text-generation-webui#basic-settings). \n",
    "Example: --auto-devices --wbits 4 --groupsize 128 --no-stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58084747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter args string: \n"
     ]
    }
   ],
   "source": [
    "from modules.shared import parser\n",
    "\n",
    "def parse_input_string(input_string):\n",
    "    input_args = input_string.split()\n",
    "    return parser.parse_args(input_args)\n",
    "\n",
    "input_string = input('Enter args string: ')\n",
    "shared.args = parse_input_string(input_string)\n",
    "# Load custom settings from a JSON file\n",
    "settings_file = None\n",
    "if shared.args.settings is not None and Path(shared.args.settings).exists():\n",
    "    settings_file = Path(shared.args.settings)\n",
    "elif Path('settings.json').exists():\n",
    "    settings_file = Path('settings.json')\n",
    "\n",
    "if settings_file is not None:\n",
    "    print(f\"Loading settings from {settings_file}...\")\n",
    "    new_settings = json.loads(open(settings_file, 'r').read())\n",
    "    for item in new_settings:\n",
    "        shared.settings[item] = new_settings[item]\n",
    "\n",
    "shared.settings['seed'] = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140895ae",
   "metadata": {},
   "source": [
    "# Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d68b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get available models\n",
    "def get_available_models():\n",
    "    if shared.args.flexgen:\n",
    "        return sorted([re.sub('-np$', '', item.name) for item in list(Path(f'{shared.args.model_dir}/').glob('*')) if item.name.endswith('-np')], key=str.lower)\n",
    "    else:\n",
    "        return sorted([re.sub('.pth$', '', item.name) for item in list(Path(f'{shared.args.model_dir}/').glob('*')) if not item.name.endswith(('.txt', '-np', '.pt', '.json'))], key=str.lower)\n",
    "\n",
    "# Get the list of available models\n",
    "available_models = get_available_models()\n",
    "\n",
    "# Set the model name\n",
    "if shared.args.model is not None:\n",
    "    shared.model_name = shared.args.model\n",
    "else:\n",
    "    if len(available_models) == 0:\n",
    "        print('No models are available! Please download at least one.')\n",
    "        sys.exit(0)\n",
    "    elif len(available_models) == 1:\n",
    "        i = 0\n",
    "    else:\n",
    "        print('The following models are available:\\n')\n",
    "        for i, model in enumerate(available_models):\n",
    "            print(f'{i+1}. {model}')\n",
    "        print(f'\\nWhich one do you want to load? 1-{len(available_models)}\\n')\n",
    "        i = int(input()) - 1\n",
    "        print()\n",
    "    shared.model_name = available_models[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9185e",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "stored as:\n",
    "shared.model\n",
    "shared.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02f6ae98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config.yaml...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "It looks like the config file at 'models\\config.yaml' is not a valid JSON file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\configuration_utils.py:658\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[1;32m--> 658\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_from_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_config_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m     config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m commit_hash\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\configuration_utils.py:746\u001b[0m, in \u001b[0;36mPretrainedConfig._dict_from_json_file\u001b[1;34m(cls, json_file)\u001b[0m\n\u001b[0;32m    745\u001b[0m     text \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m shared\u001b[38;5;241m.\u001b[39mmodel, shared\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Add Lora to the model if specified\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shared\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mlora:\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\text-generation-webui\\modules\\models.py:53\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshared\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshared\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbf16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhas_mps:\n\u001b[0;32m     55\u001b[0m         device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:441\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_copy\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    439\u001b[0m         _ \u001b[38;5;241m=\u001b[39m kwargs_copy\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 441\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    442\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    443\u001b[0m         return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    444\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_copy,\n\u001b[0;32m    447\u001b[0m     )\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:916\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    915\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 916\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 573\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    575\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\configuration_utils.py:661\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m commit_hash\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    662\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like the config file at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid JSON file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    663\u001b[0m     )\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_local:\n\u001b[0;32m    666\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading configuration file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: It looks like the config file at 'models\\config.yaml' is not a valid JSON file."
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "shared.model, shared.tokenizer = load_model(shared.model_name)\n",
    "\n",
    "# Add Lora to the model if specified\n",
    "if shared.args.lora:\n",
    "    add_lora_to_model(shared.args.lora)\n",
    "\n",
    "# Set up the tokenizer and model variables\n",
    "tokenizer = shared.tokenizer\n",
    "base_model = shared.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626420f",
   "metadata": {},
   "source": [
    "# Create Text-Generation Pipeline\n",
    "## We create a text-generation pipeline with the specified parameters:\n",
    "Feel free to change these to best fit your model/usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3857d1a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a text-generation pipeline with the specified parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mbase_model\u001b[49m, \n\u001b[0;32m      5\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m      6\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,  \u001b[38;5;66;03m# Set the device to use CUDA, remove if not using cuda\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m,\n\u001b[0;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[0;32m      9\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m     10\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mpipe)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a text-generation pipeline with the specified parameters\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer,\n",
    "    device=0,  # Set the device to use CUDA, remove if not using cuda\n",
    "    max_length=800,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbeba03",
   "metadata": {},
   "source": [
    "# The model is now loaded and can be used with langchain\n",
    "## use llm as your model variable \n",
    "llm is an instance of the HuggingFacePipeline class that wraps around a text-generation pipeline created using the pipeline() method from the transformers library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3d4a7",
   "metadata": {},
   "source": [
    "# Begginning of Langchain section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acc274",
   "metadata": {},
   "source": [
    "## install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e1093b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (0.0.139)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: gptcache>=0.1.7 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (0.1.10)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: cachetools in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
      "Requirement already satisfied: openai in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from gptcache>=0.1.7->langchain) (0.27.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from openai->gptcache>=0.1.7->langchain) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from tqdm->openai->gptcache>=0.1.7->langchain) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c87122",
   "metadata": {},
   "source": [
    "## import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27c3ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser, load_tools\n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain import SerpAPIWrapper, LLMChain, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf6d57",
   "metadata": {},
   "source": [
    "## Define which tools the agent can use to answer user queries\n",
    "In this case Wikipedia. Install it in the next cell if you dont have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2f1fac5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from wikipedia) (2.28.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\austi\\desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11706 sha256=bbf636e63df042e293b77d48da8e6e4248d6493b9d3209461040093f406f3f0f\n",
      "  Stored in directory: c:\\users\\austi\\appdata\\local\\pip\\cache\\wheels\\5e\\b6\\c5\\93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "841987a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"useful for when you need to look up information\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3ecab",
   "metadata": {},
   "source": [
    "## Set up the base template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81ffa228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: one word, Use only the name of a tool i.e \"Wikipedia\"\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f58153",
   "metadata": {},
   "source": [
    "## Create Prompt Template Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "733ca146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        return [HumanMessage(content=formatted)]\n",
    "    \n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c108d",
   "metadata": {},
   "source": [
    "## Create Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2d1c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        try:\n",
    "            # Check if agent should finish\n",
    "            if \"Final Answer:\" in llm_output:\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "\n",
    "            # Parse out the action and action input\n",
    "            regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
    "            match = re.search(regex, llm_output, re.DOTALL)\n",
    "            if not match:\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": llm_output},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "            action = match.group(1).strip()\n",
    "            action_input = match.group(2)\n",
    "\n",
    "            # Return the action and action input\n",
    "            return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "        \n",
    "        except Exception as e:\n",
    "\n",
    "            raise e\n",
    "            \n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9fd68c",
   "metadata": {},
   "source": [
    "## Create LLM chain consisting of the LLM and a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5497bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39m\u001b[43mllm\u001b[49m, prompt\u001b[38;5;241m=\u001b[39mprompt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215300c5",
   "metadata": {},
   "source": [
    "## Setup Agent with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaf8f406",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tool_names \u001b[38;5;241m=\u001b[39m [tool\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m LLMSingleActionAgent(\n\u001b[1;32m----> 3\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39m\u001b[43mllm_chain\u001b[49m, \n\u001b[0;32m      4\u001b[0m     output_parser\u001b[38;5;241m=\u001b[39moutput_parser,\n\u001b[0;32m      5\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mObservation:\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m      6\u001b[0m     allowed_tools\u001b[38;5;241m=\u001b[39mtool_names\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm_chain' is not defined"
     ]
    }
   ],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c6b56b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor\u001b[38;5;241m.\u001b[39mfrom_agent_and_tools(agent\u001b[38;5;241m=\u001b[39m\u001b[43magent\u001b[49m, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a35fee",
   "metadata": {},
   "source": [
    "# Testing\n",
    "## Ask the model a question and examine the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0e8b5c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent_executor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# question = input(\"Question: \")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhats the population of canada?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241m.\u001b[39mrun(question)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent_executor' is not defined"
     ]
    }
   ],
   "source": [
    "# question = input(\"Question: \")\n",
    "question = \"whats the population of canada?\"\n",
    "agent_executor.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394cc99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
