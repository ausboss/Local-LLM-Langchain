{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fcbe22",
   "metadata": {},
   "source": [
    "# Local Model Notebook loader\n",
    "## This is for people who want to test langchain or other agent/agi related code in a notebook\n",
    "\n",
    "\n",
    "## ‚ö†Ô∏èLlama-cpp usersü¶ô‚ö†Ô∏è\n",
    "If you are using Llama-cpp you can skip down to the llama cpp cell\n",
    "\n",
    "If your Llama uses gpu then dont skip\n",
    "# Text-generation-webui related code\n",
    "## Load Required Libraries and Modules\n",
    "The first step is to load all the required libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df86eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b411355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = [sys.argv[0]]\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "sys.path.append(str(Path().resolve().parent / \"modules\"))\n",
    "from modules import api, chat, shared, training, ui\n",
    "from modules.html_generator import chat_html_wrapper\n",
    "from modules.LoRA import add_lora_to_model\n",
    "from modules.models import load_model, load_soft_prompt\n",
    "from modules.text_generation import generate_reply, stop_everything_event\n",
    "import torch\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bf6a3",
   "metadata": {},
   "source": [
    "# Parameters and command-line flags\n",
    "\n",
    "input your command line arguments like you would when launching server.py [complete list](https://github.com/oobabooga/text-generation-webui#basic-settings)\n",
    "\n",
    "Example: --auto-devices --wbits 4 --groupsize 128 --no-stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58084747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.shared import parser\n",
    "\n",
    "def parse_input_string(input_string):\n",
    "    input_args = input_string.split()\n",
    "    return parser.parse_args(input_args)\n",
    "\n",
    "input_string = input('Enter args string: ')\n",
    "shared.args = parse_input_string(input_string)\n",
    "# Load custom settings from a JSON file\n",
    "settings_file = None\n",
    "if shared.args.settings is not None and Path(shared.args.settings).exists():\n",
    "    settings_file = Path(shared.args.settings)\n",
    "elif Path('settings.json').exists():\n",
    "    settings_file = Path('settings.json')\n",
    "\n",
    "if settings_file is not None:\n",
    "    print(f\"Loading settings from {settings_file}...\")\n",
    "    new_settings = json.loads(open(settings_file, 'r').read())\n",
    "    for item in new_settings:\n",
    "        shared.settings[item] = new_settings[item]\n",
    "\n",
    "shared.settings['seed'] = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e363387c",
   "metadata": {},
   "source": [
    "# Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get available models\n",
    "def get_available_models():\n",
    "    if shared.args.flexgen:\n",
    "        return sorted([re.sub('-np$', '', item.name) for item in list(Path(f'{shared.args.model_dir}/').glob('*')) if item.name.endswith('-np')], key=str.lower)\n",
    "    else:\n",
    "        return sorted([re.sub('.pth$', '', item.name) for item in list(Path(f'{shared.args.model_dir}/').glob('*')) if not item.name.endswith(('.txt', '-np', '.pt', '.json'))], key=str.lower)\n",
    "\n",
    "# Get the list of available models\n",
    "available_models = get_available_models()\n",
    "\n",
    "# Set the model name\n",
    "if shared.args.model is not None:\n",
    "    shared.model_name = shared.args.model\n",
    "else:\n",
    "    if len(available_models) == 0:\n",
    "        print('No models are available! Please download at least one.')\n",
    "        sys.exit(0)\n",
    "    elif len(available_models) == 1:\n",
    "        i = 0\n",
    "    else:\n",
    "        print('The following models are available:\\n')\n",
    "        for i, model in enumerate(available_models):\n",
    "            print(f'{i+1}. {model}')\n",
    "        print(f'\\nWhich one do you want to load? 1-{len(available_models)}\\n')\n",
    "        i = int(input()) - 1\n",
    "        print()\n",
    "    shared.model_name = available_models[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4771cd",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "shared.model, shared.tokenizer = load_model(shared.model_name)\n",
    "\n",
    "# Add Lora to the model if specified\n",
    "if shared.args.lora:\n",
    "    add_lora_to_model(shared.args.lora)\n",
    "\n",
    "# Set up the tokenizer and model variables\n",
    "tokenizer = shared.tokenizer\n",
    "base_model = shared.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4c523",
   "metadata": {},
   "source": [
    "# Create Text-Generation Pipeline\n",
    "## We create a text-generation pipeline with the specified parameters:\n",
    "Feel free to change these to best fit your model/usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text-generation pipeline with the specified parameters\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    max_length=800,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2996aa",
   "metadata": {},
   "source": [
    "# The model is now loaded and can be used with langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516f055",
   "metadata": {},
   "source": [
    "\n",
    "# ü¶ôLlama-cpp usersü¶ô\n",
    "## If you are just using llama-cpp then follow these steps\n",
    "\n",
    "The a folder containing the bin file should be located in the models folder \n",
    "\n",
    "Example: \"./models/Alpaca-7B-ggml-4bit-LoRA-merged/ggml-model-q4_0.bin\"\n",
    "## Install and Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8478bd",
   "metadata": {},
   "source": [
    "# Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83932f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./models\"\n",
    "import os\n",
    "# get a list of all folders in the models directory\n",
    "model_folders = [f for f in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir, f))]\n",
    "\n",
    "# print the list of model names with their index starting at 1\n",
    "for i, model_name in enumerate(model_folders):\n",
    "    print(f\"{i+1}. {model_name}\")\n",
    "\n",
    "# ask the user to select a model by number\n",
    "selected_index = int(input(\"Enter the number of the model to select: \")) - 1\n",
    "selected_model = model_folders[selected_index]\n",
    "\n",
    "# check if the selected model contains a .bin file and save the path if it does\n",
    "model_bin = None\n",
    "for file in os.listdir(os.path.join(model_dir, selected_model)):\n",
    "    if file.endswith(\".bin\"):\n",
    "        model_bin = os.path.join(model_dir, selected_model, file)\n",
    "        break\n",
    "\n",
    "if model_bin:\n",
    "    print(f\"Selected model binary: {model_bin}\")\n",
    "else:\n",
    "    print(\"No .bin file found in selected model directory.\")\n",
    "    \n",
    "llm = LlamaCpp(model_path=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3e44b",
   "metadata": {},
   "source": [
    "# Begginning of Langchain section\n",
    "I stole some of the code from [this colab](https://colab.research.google.com/drive/1VOwJpcZqOXag-ZXi-52ibOx6L5Pw-YJi#scrollTo=nu-AmhDLEK0h) that goes with [this video](https://www.youtube.com/watch?v=LbT1yp6quS8) by Patrick Loeber. I recommend subscribing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e08a9",
   "metadata": {},
   "source": [
    "## Your model variable - llm\n",
    "'llm' is an instance of the HuggingFacePipeline class that wraps around a text-generation pipeline created using the pipeline() method from the transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd85ac",
   "metadata": {},
   "source": [
    "## install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2243957",
   "metadata": {},
   "source": [
    "## import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser, load_tools\n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain import SerpAPIWrapper, LLMChain, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9e4bf",
   "metadata": {},
   "source": [
    "## Test Your llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f141b",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "LangChain helps manage and improve how prompts are used.\n",
    "\n",
    "When an app uses a language model, it doesn't directly send what the user types to the model. Instead, it creates a question or prompt based on what the user typed and sends that to the model. LangChain makes this process easier and better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a359fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Can Barack Obama have a conversation with George Washington?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb234cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Question: Can Barack Obama have a conversation with George Washington?\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "Answer: \"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.format(question=\"Can Barack Obama have a conversation with George Washington?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e81d44",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Test how well different language models and prompts work together in multi-step processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Can Barack Obama have a conversation with George Washington?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43e9fc",
   "metadata": {},
   "source": [
    "## Agents and Tools\n",
    "\n",
    "An agent is a language model that can make decisions, take actions, observe results, and repeat the process until the task is done. Agents can be very useful when used properly. Here are some concepts to know before using them:\n",
    "\n",
    "- **Tool:** A tool is a function that does a specific job, such as searching on Google, looking up data in a database, running Python code, or using other chains.\n",
    "- **LLM:** This stands for \"language model\" and it is what powers the agent.\n",
    "- **Agent:** This is the specific agent that you want to use.\n",
    "- **Tools:** You can find various tools at this link: https://python.langchain.com/en/latest/modules/agents/tools.html\n",
    "- **Agent Types:** There are different types of agents that you can use. You can find more information on them here: https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc154889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a31996",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7ef1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7412a2f7",
   "metadata": {},
   "source": [
    "# Langchain Wikipeida Searching\n",
    "## This part is very hit or miss with local LLMs but works well with chatgpt. \n",
    "Below is another agent using the wikipedia tool taht I was able to get results from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105abcee",
   "metadata": {},
   "source": [
    "## Define which tools the agent can use to answer user queries\n",
    "In this case Wikipedia. Install it in the next cell if you dont have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b12b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841987a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"useful for when you need to look up information\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317df66f",
   "metadata": {},
   "source": [
    "## Set up the base template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffa228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: one word, Use only the name of a tool i.e \"Wikipedia\"\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ea3cb",
   "metadata": {},
   "source": [
    "## Create Prompt Template Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ca146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        return [HumanMessage(content=formatted)]\n",
    "    \n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30996c3f",
   "metadata": {},
   "source": [
    "## Create Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        try:\n",
    "            # Check if agent should finish\n",
    "            if \"Final Answer:\" in llm_output:\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "\n",
    "            # Parse out the action and action input\n",
    "            regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
    "            match = re.search(regex, llm_output, re.DOTALL)\n",
    "            if not match:\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": llm_output},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "            action = match.group(1).strip()\n",
    "            action_input = match.group(2)\n",
    "\n",
    "            # Return the action and action input\n",
    "            return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "        \n",
    "        except Exception as e:\n",
    "\n",
    "            raise e\n",
    "            \n",
    "output_parser = CustomOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427825a8",
   "metadata": {},
   "source": [
    "## Create LLM chain consisting of the LLM and a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5497bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0ffd9",
   "metadata": {},
   "source": [
    "## Setup Agent with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6fa1b",
   "metadata": {},
   "source": [
    "# Testing\n",
    "## Ask the model a question and examine the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8b5c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# question = input(\"Question: \")\n",
    "question = \"whats the population of canada?\"\n",
    "agent_executor.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db40952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee18ba0",
   "metadata": {},
   "source": [
    "## Memory\n",
    "Add State to Chains and Agents.\n",
    "\n",
    "Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Can we talk about AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I'm interested in Reinforcement Learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce053521",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "If you want to use your own text data with a language model, you can use a document loader module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "\n",
    "loader = NotionDirectoryLoader(\"Notion_DB\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d726de",
   "metadata": {},
   "source": [
    "## Indexes\n",
    "\n",
    "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents\n",
    "\n",
    "- Embeddings: An embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc.\n",
    "- Text Splitters: When you want to deal with long pieces of text, it is necessary to split up that text into chunks.\n",
    "- Vectorstores: Vector databases store and index vector embeddings from NLP models to understand the meaning and context of strings of text, sentences, and whole documents for more accurate and relevant search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "  f.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "#text = \"This is a test document.\"\n",
    "#query_result = embeddings.embed_query(text)\n",
    "#doc_result = embeddings.embed_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ca91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss_index\")\n",
    "new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "docs = new_db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2572a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
