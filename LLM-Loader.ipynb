{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fcbe22",
   "metadata": {},
   "source": [
    "# Local Model Notebook loader\n",
    "## This is for people who want to test langchain or other agent/agi related code in a notebook\n",
    "\n",
    "\n",
    "## ‚ö†Ô∏èLlama-cpp usersü¶ô‚ö†Ô∏è\n",
    "If you are using Llama-cpp you can skip down to the llama cpp cell\n",
    "\n",
    "If your Llama uses gpu then dont skip\n",
    "# Text-generation-webui related code\n",
    "## Load Required Libraries and Modules\n",
    "The first step is to load all the required libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df86eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b411355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = [sys.argv[0]]\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "sys.path.append(str(Path().resolve().parent / \"modules\"))\n",
    "from modules import api, chat, shared, training, ui\n",
    "from modules.html_generator import chat_html_wrapper\n",
    "from modules.LoRA import add_lora_to_model\n",
    "from modules.models import load_model, load_soft_prompt\n",
    "from modules.text_generation import generate_reply, stop_everything_event\n",
    "import torch\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bf6a3",
   "metadata": {},
   "source": [
    "# Parameters and command-line flags\n",
    "\n",
    "input your command line arguments like you would when launching server.py [complete list](https://github.com/oobabooga/text-generation-webui#basic-settings)\n",
    "\n",
    "Example: --auto-devices --wbits 4 --groupsize 128 --no-stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58084747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.shared import parser\n",
    "\n",
    "def parse_input_string(input_string):\n",
    "    input_args = input_string.split()\n",
    "    return parser.parse_args(input_args)\n",
    "\n",
    "input_string = input('Enter args string: ')\n",
    "shared.args = parse_input_string(input_string)\n",
    "# Load custom settings from a JSON file\n",
    "settings_file = None\n",
    "if shared.args.settings is not None and Path(shared.args.settings).exists():\n",
    "    settings_file = Path(shared.args.settings)\n",
    "elif Path('settings.json').exists():\n",
    "    settings_file = Path('settings.json')\n",
    "\n",
    "if settings_file is not None:\n",
    "    print(f\"Loading settings from {settings_file}...\")\n",
    "    new_settings = json.loads(open(settings_file, 'r').read())\n",
    "    for item in new_settings:\n",
    "        shared.settings[item] = new_settings[item]\n",
    "\n",
    "shared.settings['seed'] = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e363387c",
   "metadata": {},
   "source": [
    "# Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get available models\n",
    "def get_available_models():\n",
    "    if shared.args.flexgen:\n",
    "        return sorted([re.sub('-np$', '', item.name) for item in list(Path(f'{shared.args.model_dir}/').glob('*')) if item.name.endswith('-np')], key=str.lower)\n",
    "    else:\n",
    "        return sorted([re.sub('.pth$', '', item.name) for item in list(Path(f'{shared.args.model_dir}/').glob('*')) if not item.name.endswith(('.txt', '-np', '.pt', '.json'))], key=str.lower)\n",
    "\n",
    "# Get the list of available models\n",
    "available_models = get_available_models()\n",
    "\n",
    "# Set the model name\n",
    "if shared.args.model is not None:\n",
    "    shared.model_name = shared.args.model\n",
    "else:\n",
    "    if len(available_models) == 0:\n",
    "        print('No models are available! Please download at least one.')\n",
    "        sys.exit(0)\n",
    "    elif len(available_models) == 1:\n",
    "        i = 0\n",
    "    else:\n",
    "        print('The following models are available:\\n')\n",
    "        for i, model in enumerate(available_models):\n",
    "            print(f'{i+1}. {model}')\n",
    "        print(f'\\nWhich one do you want to load? 1-{len(available_models)}\\n')\n",
    "        i = int(input()) - 1\n",
    "        print()\n",
    "    shared.model_name = available_models[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4771cd",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "shared.model, shared.tokenizer = load_model(shared.model_name)\n",
    "\n",
    "# Add Lora to the model if specified\n",
    "if shared.args.lora:\n",
    "    add_lora_to_model(shared.args.lora)\n",
    "\n",
    "# Set up the tokenizer and model variables\n",
    "tokenizer = shared.tokenizer\n",
    "base_model = shared.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4c523",
   "metadata": {},
   "source": [
    "# Create Text-Generation Pipeline\n",
    "## We create a text-generation pipeline with the specified parameters:\n",
    "Feel free to change these to best fit your model/usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text-generation pipeline with the specified parameters\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    max_length=800,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2996aa",
   "metadata": {},
   "source": [
    "# The model is now loaded and can be used with langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516f055",
   "metadata": {},
   "source": [
    "\n",
    "# ü¶ôLlama-cpp usersü¶ô\n",
    "## If you are just using llama-cpp then follow these steps\n",
    "\n",
    "The a folder containing the bin file should be located in the models folder \n",
    "\n",
    "Example: \"./models/Alpaca-7B-ggml-4bit-LoRA-merged/ggml-model-q4_0.bin\"\n",
    "## Install and Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8478bd",
   "metadata": {},
   "source": [
    "# Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83932f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./models\"\n",
    "\n",
    "# get a list of all folders in the models directory\n",
    "model_folders = [f for f in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir, f))]\n",
    "\n",
    "# print the list of model names with their index starting at 1\n",
    "for i, model_name in enumerate(model_folders):\n",
    "    print(f\"{i+1}. {model_name}\")\n",
    "\n",
    "# ask the user to select a model by number\n",
    "selected_index = int(input(\"Enter the number of the model to select: \")) - 1\n",
    "selected_model = model_folders[selected_index]\n",
    "\n",
    "# check if the selected model contains a .bin file and save the path if it does\n",
    "model_bin = None\n",
    "for file in os.listdir(os.path.join(model_dir, selected_model)):\n",
    "    if file.endswith(\".bin\"):\n",
    "        model_bin = os.path.join(model_dir, selected_model, file)\n",
    "        break\n",
    "\n",
    "if model_bin:\n",
    "    print(f\"Selected model binary: {model_bin}\")\n",
    "else:\n",
    "    print(\"No .bin file found in selected model directory.\")\n",
    "    \n",
    "llm = LlamaCpp(model_path=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3e44b",
   "metadata": {},
   "source": [
    "# Begginning of Langchain section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e08a9",
   "metadata": {},
   "source": [
    "## Your model variable - llm\n",
    "'llm' is an instance of the HuggingFacePipeline class that wraps around a text-generation pipeline created using the pipeline() method from the transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd85ac",
   "metadata": {},
   "source": [
    "## install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2243957",
   "metadata": {},
   "source": [
    "## import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser, load_tools\n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain import SerpAPIWrapper, LLMChain, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105abcee",
   "metadata": {},
   "source": [
    "## Define which tools the agent can use to answer user queries\n",
    "In this case Wikipedia. Install it in the next cell if you dont have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b12b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841987a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"useful for when you need to look up information\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317df66f",
   "metadata": {},
   "source": [
    "## Set up the base template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffa228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: one word, Use only the name of a tool i.e \"Wikipedia\"\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ea3cb",
   "metadata": {},
   "source": [
    "## Create Prompt Template Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ca146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        return [HumanMessage(content=formatted)]\n",
    "    \n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30996c3f",
   "metadata": {},
   "source": [
    "## Create Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        try:\n",
    "            # Check if agent should finish\n",
    "            if \"Final Answer:\" in llm_output:\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "\n",
    "            # Parse out the action and action input\n",
    "            regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
    "            match = re.search(regex, llm_output, re.DOTALL)\n",
    "            if not match:\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": llm_output},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "            action = match.group(1).strip()\n",
    "            action_input = match.group(2)\n",
    "\n",
    "            # Return the action and action input\n",
    "            return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "        \n",
    "        except Exception as e:\n",
    "\n",
    "            raise e\n",
    "            \n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427825a8",
   "metadata": {},
   "source": [
    "## Create LLM chain consisting of the LLM and a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5497bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0ffd9",
   "metadata": {},
   "source": [
    "## Setup Agent with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6fa1b",
   "metadata": {},
   "source": [
    "# Testing\n",
    "## Ask the model a question and examine the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8b5c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# question = input(\"Question: \")\n",
    "question = \"whats the population of canada?\"\n",
    "agent_executor.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
